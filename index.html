<!DOCTYPE html>
<html>

  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="description" content="Mysql to redshift loader : Data mover from MySQL to Redshift">

    <link rel="stylesheet" type="text/css" media="screen" href="stylesheets/stylesheet.css">

    <title>Mysql to redshift loader</title>
  </head>

  <body>

    <!-- HEADER -->
    <div id="header_wrap" class="outer">
        <header class="inner">
          <a id="forkme_banner" href="https://github.com/alexbuz/MySQL_To_Redshift_Loader">View on GitHub</a>

          <h1 id="project_title">Mysql to redshift loader</h1>
          <h2 id="project_tagline">Data mover from MySQL to Redshift</h2>

            <section id="downloads">
              <a class="zip_download_link" href="https://github.com/alexbuz/MySQL_To_Redshift_Loader/zipball/master">Download this project as a .zip file</a>
              <a class="tar_download_link" href="https://github.com/alexbuz/MySQL_To_Redshift_Loader/tarball/master">Download this project as a tar.gz file</a>
            </section>
        </header>
    </div>

    <!-- MAIN CONTENT -->
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">
        <h1>
<a id="mysql-to-redshift-data-loader" class="anchor" href="#mysql-to-redshift-data-loader" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>MySQL-to-Redshift-Data-Loader</h1>

<pre><code>Ground to cloud data integration tool
Used for ad-hoc query data results load from MySQL to Amazon-Redshift.
Works from Windows CLI (command line).
</code></pre>

<p>Features:</p>

<ul>
<li>Loads MySQL table (or query) data to Amazon-Redshift.</li>
<li>Data stream is compressed while loaded to S3 (and then to Redshift).</li>
<li>AWS Access Keys are not passed as arguments. </li>
<li>You can modify default Python <a href="https://github.com/alexbuz/MySQL_To_Redshift_Loader/blob/master/sources/include/extractor.py">extractor.py</a> and <a href="https://github.com/alexbuz/MySQL_To_Redshift_Loader/blob/master/sources/include/loader.py">loader.py</a>
</li>
<li>Written using Python/boto/psycopg2/PyInstaller.</li>
</ul>

<h2>
<a id="version" class="anchor" href="#version" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Version</h2>

<table>
<thead>
<tr>
<th>OS</th>
<th>Platform</th>
<th>Version</th>
</tr>
</thead>
<tbody>
<tr>
<td>Windows</td>
<td>64bit</td>
<td>[1.2 beta]</td>
</tr>
</tbody>
</table>

<h2>
<a id="purpose" class="anchor" href="#purpose" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Purpose</h2>

<ul>
<li>Stream/pipe/load MySQL table data to Amazon-Redshift.</li>
</ul>

<h2>
<a id="how-it-works" class="anchor" href="#how-it-works" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>How it works</h2>

<ul>
<li>Script connects to source MySQL DB and extracts data to temp file.</li>
<li>File is compressed and uploaded to S3 using multipart upload.</li>
<li>Optional upload to Reduced Redundancy storage (not RR by default).</li>
<li>Optional "make it public" after upload (private by default).</li>
<li>If S3 bucket doesn't exists it will be created.</li>
<li>You can control the region where new bucket is created.</li>
<li>Streamed data can be tee'd (dumped on disk) during load.</li>
<li>If not set, S3 Key defaulted to input query file name.</li>
<li>Data is loaded to Redshift from S3 using COPY command</li>
<li>Target Redshift table has to exist</li>
<li>It's a Python/boto/psycopg2 script

<ul>
<li>Boto S3 docs: <a href="http://boto.cloudhackers.com/en/latest/ref/s3.html">http://boto.cloudhackers.com/en/latest/ref/s3.html</a>
</li>
<li>psycopg2 docs: <a href="http://initd.org/psycopg/docs/">http://initd.org/psycopg/docs/</a>
</li>
</ul>
</li>
<li>Executable is created using <a href="http://www.pyinstaller.org/">pyInstaller</a>
</li>
</ul>

<h2>
<a id="audience" class="anchor" href="#audience" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Audience</h2>

<p>Database/ETL developers, Data Integrators, Data Engineers, Business Analysts, AWS Developers, SysOps</p>

<h2>
<a id="designated-environment" class="anchor" href="#designated-environment" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Designated Environment</h2>

<p>Pre-Prod (UAT/QA/DEV)</p>

<h2>
<a id="usage" class="anchor" href="#usage" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Usage</h2>

<pre><code>c:\Python35-32\PROJECTS\MySQL2redshift&gt;dist\MySQL_to_Redshift_loader.exe
#############################################################################
#MySQL-to-Redshift Data Loader (v1.2, beta, 04/05/2016 15:11:53) [64bit]
#Copyright (c): 2016 Alex Buzunov, All rights reserved.
#Agreement: Use this tool at your own risk. Author is not liable for any damages
#           or losses related to the use of this software.
################################################################################
Usage:
  set AWS_ACCESS_KEY_ID=test_key
  set AWS_SECRET_ACCESS_KEY=test_secret_key
  set PGPASSWORD=test123
  set MYSQL_CLIENT_HOME="C:\Program Files\MySQL\9.5"

  set REDSHIFT_CONNECT_STRING="dbname='***' port='5439' user='***' password='***' host='mycluster.***.redshift.amazonaws.com'"  


  mysql_to_redshift_loader.exe [&lt;mysql_query_file&gt;] [&lt;mysql_col_delim&gt;] [&lt;mysql_add_header&gt;] 
                [&lt;s3_bucket_name&gt;] [&lt;s3_key_name&gt;] [&lt;s3_use_rr&gt;] [&lt;s3_public&gt;]

    --mysql_query_file -- SQL query to execure in source MySQL db.
    --mysql_col_delim  -- CSV column delimiter for downstream(,).
    --mysql_quote   -- Enclose values in quotes (")
    --mysql_add_header -- Add header line to CSV file (False).
    --mysql_lame_duck  -- Limit rows for trial upload (1000).
    --create_data_dump -- Use it if you want to persist streamed data on your filesystem.

    --s3_bucket_name -- S3 bucket name (always set it).
    --s3_location    -- New bucket location name (us-west-2)
                Set it if you are creating new bucket
    --s3_key_name    -- CSV file name (to store query results on S3).
        if &lt;s3_key_name&gt; is not specified, the MySQL query filename (ora_query_file) will be used.
    --s3_use_rr -- Use reduced redundancy storage (False).
    --s3_write_chunk_size -- Chunk size for multipart upoad to S3 (10&lt;&lt;21, ~20MB).
    --s3_public -- Make uploaded file public (False).

    --red_to_table  -- Target Amazon-Redshit table name.
    --red_col_delim  -- CSV column delimiter for upstream(,).
    --red_quote     -- Set it if input values are quoted.
    --red_timeformat -- Timestamp format for Redshift ('MM/DD/YYYY HH12:MI:SS').
    --red_ignoreheader -- skip header in input stream

    MySQL data uploaded to S3 is always compressed (gzip).

    Boto S3 docs: http://boto.cloudhackers.com/en/latest/ref/s3.html
    psycopg2 docs: http://initd.org/psycopg/docs/

</code></pre>

<h1>
<a id="example" class="anchor" href="#example" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Example</h1>

<h3>
<a id="environment-variables" class="anchor" href="#environment-variables" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Environment variables</h3>

<p>Set the following environment variables (for all tests):</p>

<pre><code>  set AWS_ACCESS_KEY_ID=test_key
  set AWS_SECRET_ACCESS_KEY=test_secret_key
  set PGPASSWORD=test123
  set MYSQL_CLIENT_HOME="C:\Program Files\MySQL\9.5"

  set REDSHIFT_CONNECT_STRING="dbname='***' port='5439' user='***' password='***' host='mycluster.***.redshift.amazonaws.com'"  


</code></pre>

<h3>
<a id="test-load-with-data-dump" class="anchor" href="#test-load-with-data-dump" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Test load with data dump.</h3>

<p>MySQL table <code>crime_test</code> contains data from data.gov <a href="https://catalog.data.gov/dataset/crime">Crime</a> dataset.
In this example complete table <code>crime_test</code> get's uploaded to Aamzon-S3 as compressed CSV file.</p>

<p>Contents of the file <em>table_query.sql</em>:</p>

<pre><code>SELECT * FROM crime_test;

</code></pre>

<p>Also temporary dump file is created for analysis (by default there are no files created)
Use <code>-s, --create_data_dump</code> to dump streamed data.</p>

<p>If target bucket does not exists it will be created in user controlled region.
Use argument <code>-t, --s3_location</code> to set target region name</p>

<p>Contents of the file <em>test.bat</em>:</p>

<pre><code>dist-64bit\MySQL_to_redshift_loader.exe ^
-q table_query.sql ^
-d "," ^
-b test_bucket ^
-k mysql_table_export ^
-r ^
-o crime_test ^
-m "DD/MM/YYYY HH12:MI:SS" ^
-s
</code></pre>

<p>Executing <code>test.bat</code>:</p>

<pre><code>c:\Python35-32\PROJECTS\MySQL2redshift&gt;dist-64bit\MySQL_to_redshift_loader.exe -q table_query.sql -d "," -b test_bucket -k mysql_table_export -r -o crime_test -m "DD/MM/YYYY HH12:MI:SS" -s
Uploading results of "table_query.sql" to existing bucket "test_bucket"
Started reading from MySQL (1.25 sec).
Dumping data to: c:\Python35-32\PROJECTS\Ora2redshift\data_dump\table_query\test_bucket\mysql_table_export.20160408_203221.gz
1 chunk 10.0 MB [11.36 sec]
2 chunk 10.0 MB [11.08 sec]
3 chunk 10.0 MB [11.14 sec]
4 chunk 10.0 MB [11.12 sec]
5 chunk 877.66 MB [0.96 sec]
Size: Uncompressed: 40.86 MB
Size: Compressed  : 8.95 MB
Elapsed: MySQL+S3    :69.12 sec.
Elapsed: S3-&gt;Redshift :3.68 sec.
--------------------------------
Total elapsed: 72.81 sec.


</code></pre>

<h3>
<a id="modifying-default-redshift-copy-command" class="anchor" href="#modifying-default-redshift-copy-command" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Modifying default Redshift COPY command.</h3>

<p>You can modify default Redshift COPY command this script is using.</p>

<p>Open file <code>include\loader.py</code> and modify <code>sql</code> variable on line 24.</p>

<pre><code>    sql="""
COPY %s FROM '%s' 
    CREDENTIALS 'aws_access_key_id=%s;aws_secret_access_key=%s' 
    DELIMITER '%s' 
    FORMAT CSV %s 
    GZIP 
    %s 
    %s; 
    COMMIT;
    ...
</code></pre>

<h3>
<a id="download" class="anchor" href="#download" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Download</h3>

<ul>
<li><code>git clone https://github.com/alexbuz/MySQL-to-Redshift-Data-Loader</code></li>
<li>
<a href="https://github.com/alexbuz/MySQL-to-Redshift-Data-Loader/archive/master.zip">Master Release</a> -- <code>MySQL_to_redshift_loader 1.2</code>
</li>
</ul>

<h1>
<a id="faq" class="anchor" href="#faq" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>FAQ</h1>

<h4>
<a id="can-it-load-mysql-data-to-amazon-redshift-database" class="anchor" href="#can-it-load-mysql-data-to-amazon-redshift-database" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Can it load MySQL data to Amazon Redshift Database?</h4>

<p>Yes, it is the main purpose of this tool.</p>

<h4>
<a id="can-developers-integrate-mysql-to-redshift-data-loader-into-their-etl-pipelines" class="anchor" href="#can-developers-integrate-mysql-to-redshift-data-loader-into-their-etl-pipelines" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Can developers integrate <code>MySQL-to-Redshift-Data-Loader</code> into their ETL pipelines?</h4>

<p>Yes. Assuming they use Python.</p>

<h4>
<a id="im-trying-to-test-your-script-how-do-i-preload-data-crimecsv-into-source-mysql-db" class="anchor" href="#im-trying-to-test-your-script-how-do-i-preload-data-crimecsv-into-source-mysql-db" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>I'm trying to test your script. How do I preload data Crime.csv into source MySQL db?</h4>

<p>You can use <a href="https://github.com/data-buddy/Databuddy/releases/tag/0.3.7">CSV loader for MySQL</a></p>

<h4>
<a id="how-to-increase-load-speed" class="anchor" href="#how-to-increase-load-speed" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>How to increase load speed?</h4>

<p>Input data stream is getting compressed before upload to S3. So not much could be done here.
You may want to run it closer to source or target endpoints for better performance.</p>

<h4>
<a id="what-are-the-other-ways-to-move-large-amounts-of-data-from-mysql-to-redshift" class="anchor" href="#what-are-the-other-ways-to-move-large-amounts-of-data-from-mysql-to-redshift" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>What are the other ways to move large amounts of data from MySQL to Redshift?</h4>

<p>You can write a sqoop script that can be scheduled with Data Pipeline.</p>

<h4>
<a id="does-it-create-temporary-data-file" class="anchor" href="#does-it-create-temporary-data-file" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Does it create temporary data file?</h4>

<p>No</p>

<h4>
<a id="can-i-log-transfered-data-for-analysis" class="anchor" href="#can-i-log-transfered-data-for-analysis" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Can I log transfered data for analysis?</h4>

<p>Yes, Use <code>-s, --create_data_dump</code> to dump streamed data.</p>

<h4>
<a id="explain-first-step-of-data-transfer" class="anchor" href="#explain-first-step-of-data-transfer" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Explain first step of data transfer?</h4>

<p>The query file you provided is used to select data form target MySQL server.
Stream is compressed before load to S3.</p>

<h4>
<a id="explain-second-step-of-data-transfer" class="anchor" href="#explain-second-step-of-data-transfer" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Explain second step of data transfer?</h4>

<p>Compressed data is getting uploaded to S3 using multipart upload protocol.</p>

<h4>
<a id="explain-third-step-of-data-load-how-data-is-loaded-to-amazon-redshift" class="anchor" href="#explain-third-step-of-data-load-how-data-is-loaded-to-amazon-redshift" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Explain third step of data load. How data is loaded to Amazon Redshift?</h4>

<p>You Redshift cluster has to be open to the world (accessible via port 5439 from internet).
It uses MySQL COPY command to load file located on S3 into Redshift table.</p>

<h4>
<a id="what-technology-was-used-to-create-this-tool" class="anchor" href="#what-technology-was-used-to-create-this-tool" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>What technology was used to create this tool</h4>

<p>I used psql.exe, Python, Boto to write it.
Boto is used to upload file to S3. 
<code>psql.exe</code> is used to spool data to compressor pipe.
psycopg2 is used to establish ODBC connection with Redshift clusted and execute <code>COPY</code> command.</p>

<h4>
<a id="why-dont-you-use-odbc-driver-for-redshift-to-insert-data" class="anchor" href="#why-dont-you-use-odbc-driver-for-redshift-to-insert-data" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Why don't you use ODBC driver for Redshift to insert data?</h4>

<p>From my experience it's much slower that COPY command.
It's 10x faster to upload CSV file to Amazon-S3 first and then run COPY command.
You can still use ODBC for last step.
If you are a Java shop, take a look at Progress <a href="https://www.progress.com/blogs/booyah-amazon-redshift-challenge-loaded-in-8-min-oow14">JDBC Driver</a>.
They claim it can load 1 mil records in 6 min.</p>

<h4>
<a id="what-would-be-my-mysql-to-redshift-migration-strategy" class="anchor" href="#what-would-be-my-mysql-to-redshift-migration-strategy" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>What would be my MySQL-to-Redshift migration strategy?</h4>

<ul>
<li>Size the database</li>
<li>Network</li>
<li>Version of MySQL</li>
<li>MySQL clinet (psql.exe) availability</li>
<li>Are you doing it in one step or multiple iterations?</li>
</ul>

<h4>
<a id="do-you-use-psql-to-execute-copy-command-against-redshift" class="anchor" href="#do-you-use-psql-to-execute-copy-command-against-redshift" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Do you use psql to execute COPY command against Redshift?</h4>

<p>No. I use <code>psycopg2</code> python module (ODBC).</p>

<h4>
<a id="why-are-you-uploading-extracted-data-to-s3-whould-it-be-easier-to-just-execute-copy-command-for-local-spool-file" class="anchor" href="#why-are-you-uploading-extracted-data-to-s3-whould-it-be-easier-to-just-execute-copy-command-for-local-spool-file" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Why are you uploading extracted data to S3? whould it be easier to just execute COPY command for local spool file?</h4>

<p>As of now you cannot load from local file. You can use COPY command with Amazon Redshift, but only with files located on S3.
If you are loading CSV file from Windows command line - take a look at <a href="https://github.com/alexbuz/CSV_Loader_For_Redshift">CSV_Loader_For_Redshift</a></p>

<h4>
<a id="can-i-modify-default-psql-copy-command" class="anchor" href="#can-i-modify-default-psql-copy-command" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Can I modify default psql COPY command?</h4>

<p>Yes. Edit include/loader.py and add/remove COPY command options</p>

<p>Other options you may use:</p>

<pre><code>COMPUPDATE OFF
EMPTYASNULL
ACCEPTANYDATE
ACCEPTINVCHARS AS '^'
GZIP
TRUNCATECOLUMNS
FILLRECORD
DELIMITER '$DELIM'
REMOVEQUOTES
STATUPDATE ON
MAXERROR AS $MaxERROR
</code></pre>

<h4>
<a id="does-it-delete-file-from-s3-after-upload" class="anchor" href="#does-it-delete-file-from-s3-after-upload" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Does it delete file from S3 after upload?</h4>

<p>No</p>

<h4>
<a id="does-it-create-target-redshift-table" class="anchor" href="#does-it-create-target-redshift-table" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Does it create target Redshift table?</h4>

<p>By default no, but using <code>include\loader.py</code> you can extend default functionality and code in target table creation.</p>

<h4>
<a id="where-are-the-sources" class="anchor" href="#where-are-the-sources" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Where are the sources?</h4>

<p>Sources are <a href="https://github.com/alexbuz/MySQL_To_Redshift_Loader/tree/master/sources">here</a>.</p>

<h4>
<a id="can-you-modify-functionality-and-add-features" class="anchor" href="#can-you-modify-functionality-and-add-features" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Can you modify functionality and add features?</h4>

<p>Yes, please, ask me for new features.</p>

<h4>
<a id="what-other-aws-tools-youve-created" class="anchor" href="#what-other-aws-tools-youve-created" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>What other AWS tools you've created?</h4>

<ul>
<li>
<a href="https://github.com/alexbuz/Oracle-To-Redshift-Data-Loader">Oracle-To-Redshift-Data-Loader</a> - Stream Oracle data to Amazon-Redshift.</li>
<li>
<a href="https://github.com/alexbuz/Oracle_To_S3_Data_Uploader">Oracle_To_S3_Data_Uploader</a> - Stream Oracle data to Amazon- S3.</li>
<li>
<a href="https://github.com/alexbuz/CSV_Loader_For_Redshift/blob/master/README.md">CSV_Loader_For_Redshift</a> - Append CSV data to Amazon-Redshift from Windows.</li>
<li>
<a href="https://github.com/alexbuz/S3_Sanity_Check/blob/master/README.md">S3_Sanity_Check</a> - let's you <code>ping</code> Amazon-S3 bucket to see if it's publicly readable.</li>
<li>
<a href="https://github.com/alexbuz/EC2_Metrics_Plotter/blob/master/README.md">EC2_Metrics_Plotter</a> - plots any CloudWatch EC2 instance  metric stats.</li>
<li>
<a href="https://github.com/alexbuz/S3_File_Uploader/blob/master/README.md">S3_File_Uploader</a> - uploads file from Windows to S3.</li>
</ul>

<h4>
<a id="do-you-have-any-aws-certifications" class="anchor" href="#do-you-have-any-aws-certifications" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Do you have any AWS Certifications?</h4>

<p>Yes, <a href="https://raw.githubusercontent.com/alexbuz/FAQs/master/images/AWS_Ceritied_Developer_Associate.png">AWS Certified Developer (Associate)</a></p>

<h4>
<a id="can-you-create-similarcustom-data-tool-for-our-business" class="anchor" href="#can-you-create-similarcustom-data-tool-for-our-business" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Can you create similar/custom data tool for our business?</h4>

<p>Yes, you can PM me here or email at <code>alex_buz@yahoo.com</code>.
I'll get back to you within hours.</p>

<h3>
<a id="links" class="anchor" href="#links" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Links</h3>

<ul>
<li><a href="https://github.com/alexbuz/FAQs/blob/master/README.md">Employment FAQ</a></li>
</ul>
      </section>
    </div>

    <!-- FOOTER  -->
    <div id="footer_wrap" class="outer">
      <footer class="inner">
        <p class="copyright">Mysql to redshift loader maintained by <a href="https://github.com/alexbuz">alexbuz</a></p>
        <p>Published with <a href="https://pages.github.com">GitHub Pages</a></p>
      </footer>
    </div>

    

  </body>
</html>
